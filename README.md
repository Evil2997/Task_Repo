# Court Registry Loader (Ukraine) — 2025

Инструмент для авто-загрузки реестра стану розгляду справ, извлечения CSV из архивов и импорта данных в **SQLite** с
учётом кодировок, дедупликации и нормализации.

## Задание (формулировка)

* Источник: офиц. страница реестра; данные опубликованы архивами с CSV за 2019–2025 гг. (важно учитывать кодировку)
  .&#x20;
* Цель: реализовать **автоматическую загрузку всех CSV за 2025 год**.&#x20;
* Процесс: скачать архив → импортировать в **SQLite** → **после успешного импорта удалить архив**.&#x20;
* Типы данных подбирать по смыслу колонок; база **не должна содержать полных дублей**; по возможности — **нормализация**
  .&#x20;
* Бонус: выгрузка из БД по списку номеров дел из входного CSV (одна колонка).&#x20;
* Репозиторий: код + `requirements.txt` и **README** с описанием алгоритма.&#x20;

---

## «Алгоритм (конвейер)»

### Коротко

Страницы листаются до тех пор, пока встречаются выгрузки нужного года. Каждую найденную ZIP-ссылку обрабатываем **сразу**: скачали → распаковали → импортировали CSV → всё временное удалили. На диске одновременно лежит только один архив и его содержимое.

### Подробно, шаг за шагом

1. **Инициализация HTTP-сессии.** Создаётся один `requests.Session` с заголовками (User-Agent, Accept-Language). Это экономит время за счёт keep-alive.

2. **Постраничный поиск ZIP.**

   * Для `?page=1,2,...` загружается HTML, находятся `<a href="...zip">`.
   * Дата выгрузки берётся из «человеческого» текста в строке рядом с кнопкой скачивания (формат `dd.mm.yyyy`).
   * Если год даты совпадает с целевым — ссылка считается нужной и **отдаётся потоково** (generator `yield`).
   * Как только на странице все даты **строго меньше** целевого года, пагинация **останавливается** (ранний выход).
   * Печатаются логи вида: `[INFO] Страница N: найдено X файлов за <год>`.

3. **Загрузка и распаковка по одной ссылке.**
   Для каждой пришедшей ZIP-ссылки:

   * создаётся **временная папка** (`TemporaryDirectory`);
   * ZIP скачивается потоково (по кускам, размер задаётся параметром `chunk_size`) во временную папку;
   * архив **распаковывается** в подкаталог `extracted`.

4. **Импорт CSV.**

   * Перебираются все CSV из `extracted`; чтение с автоподбором кодировки (`utf-8-sig` → `cp1251` → `utf-8`), все поля читаются как строки без `NaN`.
   * Данные заносятся в SQLite по нормализованной схеме (`cases`, `judges`, `case_judges`, `case_events`) c `UNIQUE`-ограничениями/UPSERT, транзакционно.
   * Соединение настроено на `PRAGMA journal_mode=WAL;` + `synchronous=NORMAL` для баланса скорости/надёжности.

5. **Авто-очистка.**

   * По выходу из контекст-менеджера временная папка удаляется целиком → уносит ZIP и распакованные CSV.
   * В логах видно: `[✓] Распаковано …`, `[DB] Импортировано …`, `[CLEAN] Обработан и удалён …`.

6. **Результат.**

   * Все данные за выбранный год оказываются в `output_dir/court_registry.db`. В процессе работы SQLite может создавать служебные `*.db-wal` и `*.db-shm` (режим WAL); они управляются самой СУБД и исчезают/очищаются после чекпоинта/закрытия соединений.

### Параметры и запуск

* `--year` — целевой год (по умолчанию 2025).
* `--max-pages` — максимум страниц для сканирования (по умолчанию 50).
* `--chunk-size` — размер куска при скачивании, мегабайт (введеное значение умножиться на `1024*1024`). Рекомендуется экспериментировать в диапазоне **1 – 10 MB**.
  Все параметры прокидываются в конвейерную функцию распаковки и в загрузчик ZIP.~~

### Почему это быстро и бережно к диску

* **Ранний стоп по году**: не листаем лишние десятки страниц прошлых лет.
* **Потоковая обработка**: «нашли ссылку → сразу обработали → сразу удалили» — нет накопления больших файлов.
* **Одна HTTP-сессия + потоковое скачивание**: меньше накладных расходов сети и памяти.

---

## Что делает проект (бонус)

1. Читает входной CSV со **списком номеров дел** (одна колонка `case_number`).
2. Ищет эти дела в SQLite-БД, подтягивает:

    * атрибуты дела (`court_name`, `case_number`, `registration_date`, `type`, `description`);
    * судей: `reporting_judge` (суддя-доповідач) и `panel_judges` (прочие, в формате `роль: ПІБ`);
    * **последнее событие** по делу (дата/стадия/результат/dep/case\_proc).
3. Формирует **итоговый CSV** (`UTF-8-SIG`) со сводкой по каждому номеру; для не найденных ставит `not_found=1`.

---

## Нормализованная схема БД

Происходит разнесение исходных 13 CSV-полей по четырём таблицам:

### `cases` — дела

* `id` INTEGER PK
* `court_name` TEXT
* `case_number` TEXT
* `registration_date` DATE (`YYYY-MM-DD`)
* `type` TEXT
* `description` TEXT
  **UNIQUE(court\_name, case\_number)**

### `judges` — судьи (справочник)

* `id` INTEGER PK
* `name` TEXT UNIQUE

### `case_judges` — связь дело↔судья

* `id` INTEGER PK
* `case_id` FK → cases.id
* `judge_id` FK → judges.id
* `role` TEXT
  **UNIQUE(case\_id, judge\_id, role)**

### `case_events` — стадии/результаты

* `id` INTEGER PK
* `case_id` FK → cases.id
* `case_proc` TEXT
* `stage_date` TEXT (`YYYY-MM-DD`)
* `stage_name` TEXT
* `cause_result` TEXT
* `cause_dep` TEXT
  **UNIQUE(case\_id, stage\_date, stage\_name, cause\_result, cause\_dep)**

---

## Установка

```bash
    python3.13 -m venv .venv
    source .venv/bin/activate   # Windows: .venv\Scripts\activate
    pip install -r requirements.txt
```

---

## Запуск

```bash
    python main.py --year 2025 --chunk-size 4
```

Что произойдёт:

* проверка локальных ZIP за год → догрузка недостающих;
* распаковка;
* импорт всех CSV в БД;
* успешный архив удаляется.

Пути (можно править в `paths.py`/`config.py`):

* входные ZIP: `INPUT_DIR`
* выходная БД: `DB_PATH` (по умолчанию `output_dir/court_registry.db`)

---

## Алгоритмические детали

* **Кодировки CSV**: пробуем `utf-8-sig`, `cp1251`, `windows-1251`, `utf-8`.
* **Чтение только строками**: `dtype=str, keep_default_na=False, na_filter=False` — так пустые ячейки не превращаются в
  `NaN`, и строковые операции безопасны.
* **Даты**: вход `DD.MM.YYYY` → хранение `YYYY-MM-DD`. Пустые — `""` (для участия в `UNIQUE`).
* **Парсинг судей**: строки вида `«роль: ПІБ»`, множ. значения разделены `;`.
* **Дедуп**: `UNIQUE` на ключевых комбинациях (дело, связка дело↔судья, событие).
* **Надёжность**: импорт транзакционно; архив удаляется только после полного успеха его CSV.

---

## Алгоритмические детали (бонус)

* **Вход**: `input_cases.csv` читается как строки (`dtype=str`, `keep_default_na=False`, `na_filter=False`), кодировка —
  авто: `utf-8-sig` → `cp1251` → `utf-8`. Определяется колонка `case_number` (или берётся первый столбец). Значения
  триммируются, пустые отбрасываются, дубликаты удаляются с сохранением порядка.
* **Поиск дел**: батчами по \~500 значений выполняется `SELECT` из `cases` с `WHERE case_number IN (...)`. Строятся:

    * `case_id → данные cases`,
    * `case_number → [case_id, ...]` (на случай нескольких судов для одного номера).
* **Судьи**: для найденных `case_id` выполняется `JOIN case_judges → judges`. Роли, начинающиеся с `суддя-доповідач`,
  агрегируются в `reporting_judge`; остальные — в `panel_judges` как `роль: ПІБ`, объединение через `"; "`.
* **Последнее событие**: из `case_events` выбираются все записи по `case_id`. «Последнее» определяется максимизацией
  ключа `(stage_date, id)`, где пустая дата трактуется как `"0000-00-00"`. В результат попадают: `last_stage_date`,
  `last_stage_name`, `last_cause_result`, `last_cause_dep`, `last_case_proc`.
* **Формирование строк**:

    * для не найденных номеров — одна строка с `case_number` и `not_found=1`;
    * для найденных — по строке на каждую пару `(court_name, case_number)` (если один номер встречается в разных судах).
* **Выход**: сохраняется `output_csv` (по умолчанию разделитель `,`, кодировка `UTF-8-SIG`) с колонками:

  ```
  court_name, case_number, registration_date, type, description,
  reporting_judge, panel_judges,
  last_stage_date, last_stage_name, last_cause_result, last_cause_dep, last_case_proc,
  not_found
  ```

---

## Лицензия

MIT.

---

### Ограничения текущей версии

Инструмент **не универсален** и не предназначен для расширенного использования в других задачах.  
Он создан исключительно для решения текущей задачи и **не поддерживает**:

- Сканирование произвольного диапазона страниц.
- Сканирование данных за годы, отличные от последнего.
- Сканирование по иным фильтрам.
- Сбор и обработку файлов других форматов (кроме ZIP → CSV).

---
